{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":748156,"sourceType":"datasetVersion","datasetId":239},{"sourceId":3923312,"sourceType":"datasetVersion","datasetId":2329628},{"sourceId":3923315,"sourceType":"datasetVersion","datasetId":2329629},{"sourceId":3923325,"sourceType":"datasetVersion","datasetId":2329633}],"dockerImageVersionId":30209,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from keras.layers import Conv3D, MaxPool3D, Flatten, Dense\nfrom keras.layers import Dropout, Input, BatchNormalization\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom plotly.offline import iplot, init_notebook_mode\nfrom keras.losses import categorical_crossentropy\nfrom tensorflow.keras.optimizers import Adadelta, Adam\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential, load_model\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\nimport plotly.graph_objs as go\nfrom matplotlib.pyplot import cm\nimport matplotlib.pyplot as plt\nfrom keras.models import Model\nimport numpy as np\nimport keras\nimport h5py\n\ninit_notebook_mode(connected=True)\n%matplotlib inline\nplt.style.use('fivethirtyeight')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-07-23T17:01:52.157663Z","iopub.execute_input":"2022-07-23T17:01:52.158117Z","iopub.status.idle":"2022-07-23T17:01:52.17576Z","shell.execute_reply.started":"2022-07-23T17:01:52.158071Z","shell.execute_reply":"2022-07-23T17:01:52.173921Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#E98C00\">Loading the Dataset</span> üéÜ","metadata":{}},{"cell_type":"code","source":"with h5py.File('../input/3d-mnist/full_dataset_vectors.h5', 'r') as dataset:\n    x_train, x_test = dataset[\"X_train\"][:], dataset[\"X_test\"][:]\n    y_train, y_test = dataset[\"y_train\"][:], dataset[\"y_test\"][:]\n\nprint (\"x_train shape: \", x_train.shape)\nprint (\"y_train shape: \", y_train.shape)\n\nprint (\"x_test shape:  \", x_test.shape)\nprint (\"y_test shape:  \", y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-07-23T17:05:25.267935Z","iopub.execute_input":"2022-07-23T17:05:25.269425Z","iopub.status.idle":"2022-07-23T17:05:26.715878Z","shell.execute_reply.started":"2022-07-23T17:05:25.269373Z","shell.execute_reply":"2022-07-23T17:05:26.714119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#E98C00\">Visualize Examples</span> üìä\n***\n\nüìå - **Let's look at the first 5 digits in our dataset in 3D space!**","metadata":{}},{"cell_type":"code","source":"with h5py.File(\"../input/3d-mnist/train_point_clouds.h5\", \"r\") as points_dataset:        \n    digits = []\n    for i in range(10):\n        digit = (points_dataset[str(i)][\"img\"][:], \n                 points_dataset[str(i)][\"points\"][:], \n                 points_dataset[str(i)].attrs[\"label\"]) \n        digits.append(digit)\n\nindex = -1\n\nfor j in [1, -2, 5, -3, -1, 0]:\n    x_c = [r[0] for r in digits[j][1]]\n    y_c = [r[1] for r in digits[j][1]]\n    z_c = [r[2] for r in digits[j][1]]\n    \n    index += 1\n    \n    trace1 = go.Scatter3d(x = x_c, y = y_c, z = z_c, mode = \"markers\", marker_symbol = \"circle-open\",\n                          marker = dict(size = 3, color = y_c, colorscale = \"Phase\", opacity = 1))\n\n    data = [trace1]\n    \n    layout = go.Layout(height = 600, width = 900, template = \"plotly_dark\",\n                       title= f\"Digit: {index} in 3D space\")\n    \n    fig = go.Figure(data = data, layout = layout)\n\n    fig.update_layout(title = f\"Digit: {index} in 3D space\", font = dict(family = \"PT Sans\", size = 15))\n    \n    iplot(fig)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#E98C00\">Converting the input to 4D shape to use 3D convolution</span> üëæ\n***\n\nüìå - **3D convolutions applies a 3 dimensional filter to the dataset and the filter moves 3-direction (x, y, z) to calcuate the low level feature representations.** Their output shape is a 3 dimensional volume space such as cube or cuboid. \n\nüìå - They are helpful in event detection in videos, 3D medical images etc. They are not limited to 3d space but can also be applied to 2d space inputs such as images.","metadata":{}},{"cell_type":"code","source":"# Introduce the channel dimension in the input dataset \nxtrain = np.ndarray((x_train.shape[0], 4096, 3))\nxtest = np.ndarray((x_test.shape[0], 4096, 3))\n\n# Translate data to color\ndef add_rgb_dimension(array):\n    scalar_map = cm.ScalarMappable(cmap = \"Oranges\")\n    return scalar_map.to_rgba(array)[:, : -1]\n\n# Iterate through train and test, add the RGB dimension \nfor i in range(x_train.shape[0]):\n    xtrain[i] = add_rgb_dimension(x_train[i])\nfor i in range(x_test.shape[0]):\n    xtest[i] = add_rgb_dimension(x_test[i])\n\n# Convert to 1 + 4D space (1st argument represents number of rows in the dataset)\nxtrain = xtrain.reshape(x_train.shape[0], 16, 16, 16, 3)\nxtest = xtest.reshape(x_test.shape[0], 16, 16, 16, 3)\n\n# Convert target variable into one-hot\ny_train = to_categorical(y_train, 10)","metadata":{"execution":{"iopub.status.busy":"2022-07-23T17:05:28.870542Z","iopub.execute_input":"2022-07-23T17:05:28.871034Z","iopub.status.idle":"2022-07-23T17:05:37.900537Z","shell.execute_reply.started":"2022-07-23T17:05:28.871002Z","shell.execute_reply":"2022-07-23T17:05:37.899184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtrain.shape, y_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#5fcac7\">Convolutional Layers</span> üßÖ\n***\n\nüìå - **Although we are working with 3D MNIST in this instance, I have found this visualization of convolutional layers to be quite helpful for gaining intuition on the inner workings of a CNN.** Image credit: https://www.youtube.com/watch?v=pj9-rr1wDhM","metadata":{}},{"cell_type":"code","source":"import IPython\nfrom IPython.display import display\nfrom PIL import Image\nIPython.display.Image(filename='../input/convnn/conv-nn.png')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#5fcac7\">Feature Maps</span> üó∫\n***\n\nüìå - Convolutional layers have a set of **independent filters** whose depth is equal to the input. Other dimensions can be set manually. If a kernel is initialized with values in a speciifc configuration, they can be used to transform an input image and find various **patterns**. These filters when convolved over the input image produce **feature maps**. Image credit: https://www.youtube.com/watch?v=pj9-rr1wDhM","metadata":{}},{"cell_type":"code","source":"IPython.display.Image(filename='../input/featuremap/feature-map.png')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#5fcac7\">Max Pooling</span> üé±\n***\n\nüìå - **Max Pooling reduces the spatial dimensions of the feature maps before traversing through the fully connected layers.** Image credit: https://www.youtube.com/watch?v=pj9-rr1wDhM","metadata":{}},{"cell_type":"code","source":"IPython.display.Image(filename='../input/maxpool/max-pooling.png')","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#E98C00\">Building 3D Convolutional Neural Network</span> üß†\n***\n\n**Lets create the model architecture. The architecture is described below:**\n\nüìå - **Input and Output layers:**\n\n- One input layer with dimension (16, 16, 16, 3) and output layer with dimension 10.\n\nüìå - **Convolutions:**\n\n- Apply 4 convolutional layers with increasing order of filter size (16, 32, 64, 128) and fixed kernel size = (3, 3, 3).\n\n- Apply 2 max pooling layers, one after 2nd convolutional layer and one after fourth convolutional layer.\n\nüìå - **MLP architecture:**\n\n- Batch normalization on convolutional architecture.\n\n- Dense layers with 2 layers followed by dropout to avoid overfitting.","metadata":{}},{"cell_type":"code","source":"input_layer = Input((16, 16, 16, 3)) # Input image dimensions\n\n# Building a 3D ConvNet\n# CNN is derived from the convolutional operator (dot product of 2 functions to produce a 3rd function)\n\nmodel = Sequential() # Sequential Keras API which is a linear stack of layers\n\nmodel.add(Conv3D(filters = 16, # The number of filters (Kernels) used with this layer\n                 \n                 kernel_size = (3, 3, 3), # The dimensions of the feature map\n                 \n                 activation = \"relu\", # Activation function - Rectified Linear Unit (ReLU)\n                 \n                 strides = 1, # How much the window (feature map) shifts by in each of the dimensions\n                 \n                 padding = \"same\", # When stride = 1, output spatial shape is the same as input spatial shape\n                 \n                 use_bias = False, # If use_bias is True, a bias vector is created and added to the outputs.\n                 \n                 # There are two conventions for shapes of images tensors: the channels-last convention \n                 # (used by TensorFlow) and the channels-first convention (used by Theano).\" \n                 # Deep Learning with Python - Fran√ßois Chollet\n                 data_format = \"channels_last\"))\n\n# Scales down outliers and forces the network to learn features in a distributed way\n# By not relying too much on any particular weight, this helps the model better generalize the images\nmodel.add(BatchNormalization())\n\nmodel.add(Conv3D(filters = 32, kernel_size = (3, 3, 3), activation = \"relu\", \n                 use_bias = False, strides = 1, padding = \"same\", data_format = \"channels_last\"))\n\nmodel.add(BatchNormalization())\n\n# Max Pooling reduces the spatial dimensions of the feature maps before the fully connected layers\nmodel.add(MaxPool3D(pool_size = (2, 2, 2))) # the pool_size (2, 2, 2) halves the size of its input\n\nmodel.add(Conv3D(filters = 64, kernel_size = (3, 3, 3), activation = \"relu\", \n                 use_bias = False, strides = 1, padding = \"same\", data_format = \"channels_last\"))\n          \nmodel.add(BatchNormalization())\n        \nmodel.add(Conv3D(filters = 128, kernel_size = (3, 3, 3), activation = \"relu\", \n                 use_bias = False, strides = 1, padding = \"same\", data_format = \"channels_last\"))\n\nmodel.add(BatchNormalization())\nmodel.add(MaxPool3D(pool_size = (2, 2, 2))) # the pool_size (2, 2, 2) halves the size of its input\n\n# To help avoid overfitting we can add Dropout. \n# This randomly drops some percentage of neurons, and thus the weights become re-aligned\nmodel.add(Dropout(0.2)) # No more BatchNorm after this layer because we introduce Dropout\n\n# Finally, we can add a flatten layer to map the input to a 1D vector\n# We then add fully connected layers after some convolutional/pooling layers.\n\nmodel.add(Flatten())\nmodel.add(Dense(4096, activation = \"relu\"))\nmodel.add(Dropout(0.45))\nmodel.add(Dense(1024, activation = \"relu\"))\nmodel.add(Dropout(0.45))\nmodel.add(Dense(10, activation = \"softmax\")) # activation function for Multi-Class Classification","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2022-07-23T17:05:39.953435Z","iopub.execute_input":"2022-07-23T17:05:39.953858Z","iopub.status.idle":"2022-07-23T17:05:39.999261Z","shell.execute_reply.started":"2022-07-23T17:05:39.953826Z","shell.execute_reply":"2022-07-23T17:05:39.997944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#E98C00\">Compiling the Model</span> ‚öôÔ∏è\n***\n\nüìå - **Step 1 - Specify the optimizer used by the model:** \n\n- We will be using the Adam optimizer in this instance but please refer to the [Keras documentation](https://keras.io/api/optimizers/) for a comprehensive list of optimizers available.\n\n- The Adam optimizer (Adaptive Moment Estimation) is an improved version of gradient descent. Interestingly, the Adam algorithm doesn't use a single global learning rate Alpha. It uses a different learning rates for every single parameter of your model.\n\n- The intuition behind the Adam algorithm is if the coefficients (weights and bias) keep moving in roughly the same direction, Adam will increase the learning rate for that parameter. In other words, let's go faster in that direction. -->\n\nüìå - **Step 2 - Specify the loss function of the model:** \n\n- For Binary Classification we use \"binary_crossentropy\" and for Multi-class Classification we use \"categorical_crossentropy\".\n\nüìå - **Step 3 - Specify metric to evaluate model performance:**\n\n- We will be using the accuracy metric but please refer to the [Keras documentation](https://keras.io/api/metrics/) for a comprehensive list of metrics available.","metadata":{}},{"cell_type":"code","source":"optimizer = Adam(learning_rate = 0.0001) # Optimizer\n# Adam will increase the learning rate when the coefficients are moving in roughly the same direction","metadata":{"execution":{"iopub.status.busy":"2022-07-23T17:05:45.724698Z","iopub.execute_input":"2022-07-23T17:05:45.725147Z","iopub.status.idle":"2022-07-23T17:05:45.732309Z","shell.execute_reply.started":"2022-07-23T17:05:45.725112Z","shell.execute_reply":"2022-07-23T17:05:45.730556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Compiling the model\nmodel.compile(optimizer = optimizer, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2022-07-23T17:05:46.52954Z","iopub.execute_input":"2022-07-23T17:05:46.52993Z","iopub.status.idle":"2022-07-23T17:05:46.543719Z","shell.execute_reply.started":"2022-07-23T17:05:46.529898Z","shell.execute_reply":"2022-07-23T17:05:46.542288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#E98C00\">Learning Rate Scheduler</span> üìÖ\n***\n\nüìå - At the beginning of every epoch, this callback retrieves the updated learning rate value from the schedule function provided at __init__, with the current epoch and current learning rate, and applies the updated learning rate on the optimizer. [Source](https://keras.io/api/callbacks/learning_rate_scheduler/)\n\nüìå - A learning rate that is too high will make the learning jump over minima but a too low learning rate will either take too long to converge or get stuck in an undesirable local minimum.\n\n**Essentially, the Learning Rate Scheduler outputs a new learning rate after each epoch iteration.**","metadata":{}},{"cell_type":"code","source":"reduce_lr = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)","metadata":{"execution":{"iopub.status.busy":"2022-07-23T17:05:48.220394Z","iopub.execute_input":"2022-07-23T17:05:48.221675Z","iopub.status.idle":"2022-07-23T17:05:48.228672Z","shell.execute_reply.started":"2022-07-23T17:05:48.221628Z","shell.execute_reply":"2022-07-23T17:05:48.22728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#E98C00\">More Callbacks</span> üì≤\n***\n\nüìå - **A callback is an object that can perform actions at various stages of training (e.g. at the start or end of an epoch, before or after a single batch, etc).** [Source](https://keras.io/api/callbacks/) ","metadata":{}},{"cell_type":"code","source":"filepath = \"best_weight.h5\"\npatience_earlystop = 7\npatience_ReduceLROnPlateau = 3\n\nmcp = ModelCheckpoint(filepath, monitor = \"val_loss\", mode = \"min\", \n                      save_best_only = True, save_weights_only = True, verbose = 1)\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor = \"val_acc\", patience = patience_ReduceLROnPlateau, \n                                            verbose = 1, factor = 0.5, min_lr = 1e-5)","metadata":{"execution":{"iopub.status.busy":"2022-07-23T17:05:49.776731Z","iopub.execute_input":"2022-07-23T17:05:49.778675Z","iopub.status.idle":"2022-07-23T17:05:49.787962Z","shell.execute_reply.started":"2022-07-23T17:05:49.778621Z","shell.execute_reply":"2022-07-23T17:05:49.786354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#E98C00\">Fitting the Model</span> üõ†","metadata":{}},{"cell_type":"code","source":"# Hyper Parameters\nBATCH_SIZE = 64\nEPOCHS = 50\nhistory = model.fit(x = xtrain, y = y_train, validation_data = (x_test, y_test),\n                    batch_size = BATCH_SIZE, epochs = EPOCHS,\n                    validation_split = 0.3, verbose = 1,\n                    callbacks = [reduce_lr, learning_rate_reduction, mcp])","metadata":{"execution":{"iopub.status.busy":"2022-07-23T17:05:51.784741Z","iopub.execute_input":"2022-07-23T17:05:51.785645Z","iopub.status.idle":"2022-07-23T17:09:15.460961Z","shell.execute_reply.started":"2022-07-23T17:05:51.785611Z","shell.execute_reply":"2022-07-23T17:09:15.459561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# <span style=\"color:#E98C00\">Training and Validation Curves</span> üìâ üìà","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize = (12, 6))\naccuracy = history.history[\"accuracy\"]\nval_accuracy = history.history[\"val_accuracy\"]\nloss = history.history[\"loss\"]\nval_loss = history.history[\"val_loss\"]\nepochs = range(len(accuracy))\nplt.plot(epochs, accuracy, 'b', label = \"Training accuracy\")\nplt.plot(epochs, val_accuracy, 'r', label = \"Validation accuracy\")\nplt.title(\"Training and validation accuracy\")\nplt.legend()\nplt.figure(figsize = (12, 6))\nplt.plot(epochs, loss, 'b', label = \"Training loss\")\nplt.plot(epochs, val_loss, 'r', label = \"Validation loss\")\nplt.title(\"Training and validation loss\")\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-23T17:09:15.463677Z","iopub.execute_input":"2022-07-23T17:09:15.464466Z","iopub.status.idle":"2022-07-23T17:09:15.981703Z","shell.execute_reply.started":"2022-07-23T17:09:15.464419Z","shell.execute_reply":"2022-07-23T17:09:15.980285Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Thank you for viewing this notebook. Please feel free to provide any feedback!**\n\n# <span style=\"color:#E98C00\">Please upvote if you like the work in this notebook!</span> üëç","metadata":{}}]}